{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.metrics import accuracy_score, classification_report \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import learning_curve\n",
    "import joblib \n",
    "import seaborn as sns \n",
    "import warnings \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"hypertension_dataset.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing One Hot Coding to Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping for categorical columns\n",
    "category_mappings = {\n",
    "    'Smoking_Status': {'Never': 0, 'Current': 1},\n",
    "    'Physical_Activity_Level': {'Low': 0, 'Moderate': 1, 'High': 2},\n",
    "    'Family_History': {'No': 0, 'Yes': 1},\n",
    "    'Gender': {'Female': 0, 'Male': 1},\n",
    "    'Education_Level': {'Primary': 0, 'Secondary': 1, 'Tertiary': 2},\n",
    "    'Employment_Status': {'Unemployed': 0, 'Employed': 1, 'Retired': 2},\n",
    "    'Hypertension': {'Low': 0, 'High': 1}\n",
    "}\n",
    "\n",
    "# Apply the mappings\n",
    "df_mapped = df.copy()\n",
    "\n",
    "for column, mapping in category_mappings.items():\n",
    "    if column in df_mapped.columns:\n",
    "        df_mapped[column] = df_mapped[column].map(mapping)\n",
    "\n",
    "# Display the mapped dataframe\n",
    "df_mapped.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_values = df.duplicated().sum()\n",
    "duplicated_values\n",
    "\n",
    "df = df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers using boxplots\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numerical_columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f'Boxplot for {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Proportion of Hypertension Values\")\n",
    "df['Hypertension'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Age Grouping (bins)\n",
    "bins = [0, 30, 45, 60, 100]\n",
    "labels = ['Young', 'Middle-Aged', 'Senior', 'Very Senior']\n",
    "df['Age_Group'] = pd.cut(df['Age'], bins=bins, labels=labels)\n",
    "\n",
    "# 2. Create BMI Categories (bins)\n",
    "bmi_bins = [0, 18.5, 24.9, 29.9, 40, 100]\n",
    "bmi_labels = ['Underweight', 'Normal', 'Overweight', 'Obese', 'Very Obese']\n",
    "df['BMI_Category'] = pd.cut(df['BMI'], bins=bmi_bins, labels=bmi_labels)\n",
    "\n",
    "# 3. Create Cholesterol to Age Ratio\n",
    "df['Cholesterol_to_Age'] = df['Cholesterol'] / df['Age']\n",
    "\n",
    "# 4. Interaction Features (Systolic_BP * Diastolic_BP)\n",
    "df['BP_Interaction'] = df['Systolic_BP'] * df['Diastolic_BP']\n",
    "\n",
    "# 5. Interaction Feature (BMI * Cholesterol)\n",
    "df['BMI_Cholesterol_Interaction'] = df['BMI'] * df['Cholesterol']\n",
    "\n",
    "# 6. Polynomial Features for Cholesterol (degree 2)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "cholesterol_poly = poly.fit_transform(df[['Cholesterol']])\n",
    "\n",
    "# Convert to DataFrame and add it to the dataset\n",
    "cholesterol_poly_df = pd.DataFrame(cholesterol_poly, columns=poly.get_feature_names_out(['Cholesterol']))\n",
    "df = pd.concat([df, cholesterol_poly_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical columns\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = numerical_columns.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = df.drop('Hypertension', axis=1)  # Drop the target variable\n",
    "y = df['Hypertension']  # Target variable\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select only numerical columns\n",
    "numerical_columns = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_test_scaled = scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "# If you need to reattach the non-numeric columns (e.g., categorical) to the scaled data:\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=numerical_columns, index=X_train.index)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=numerical_columns, index=X_test.index)\n",
    "\n",
    "# If you want to combine with non-numeric columns later (e.g., one-hot encoded columns), you can do this:\n",
    "X_train_final = pd.concat([X_train_scaled_df, X_train.drop(columns=numerical_columns)], axis=1)\n",
    "X_test_final = pd.concat([X_test_scaled_df, X_test.drop(columns=numerical_columns)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Chi-Square Test (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature 'Country' is not significant (p >= 0.05).\n",
      "The feature 'Smoking_Status' is not significant (p >= 0.05).\n",
      "The feature 'Physical_Activity_Level' is not significant (p >= 0.05).\n",
      "The feature 'Family_History' is not significant (p >= 0.05).\n",
      "The feature 'Diabetes' is not significant (p >= 0.05).\n",
      "The feature 'Gender' is not significant (p >= 0.05).\n",
      "The feature 'Education_Level' is not significant (p >= 0.05).\n",
      "The feature 'Employment_Status' is not significant (p >= 0.05).\n",
      "The feature 'Age_Group' is not significant (p >= 0.05).\n",
      "The feature 'BMI_Category' is not significant (p >= 0.05).\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Iterate over each column in the dataset\n",
    "for column in df.columns:\n",
    "    if column != 'Hypertension':  # Skip the target variable itself\n",
    "        # Check if the column is categorical\n",
    "        if df[column].apply(type).iloc[0] == str:  # Check if the column is of string type (categorical data)\n",
    "            # Create a contingency table for the categorical feature and the target variable\n",
    "            contingency_table = pd.crosstab(df[column], df['Hypertension'])\n",
    "\n",
    "            # Perform Chi-Square test\n",
    "            chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "            # Check p-value to determine if the feature is statistically significant\n",
    "            if p < 0.05:\n",
    "                print(f\"The feature '{column}' is important (p < 0.05).\")\n",
    "            else:\n",
    "                print(f\"The feature '{column}' is not significant (p >= 0.05).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Perform Statistical Tests (Continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature 'Age' is not significant using ANOVA (p >= 0.05).\n",
      "The feature 'BMI' is not significant using ANOVA (p >= 0.05).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r6/cj5qz5892bqf90ldcn4ym7tm0000gn/T/ipykernel_1245/3983767129.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mp_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mskew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get the skewness of the column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Check if skewness is less than 1, indicating normal distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskew_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# If skew is not high, assume normal distribution for ANOVA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mp_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manova_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hypertension'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtest_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ANOVA\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1578\u001b[0m             \u001b[0;34mf\"\u001b[0m\u001b[0;34mThe truth value of a \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m is ambiguous. \u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "from scipy.stats import f_oneway, kruskal\n",
    "\n",
    "# Function to perform ANOVA for continuous variables\n",
    "def anova_test(column, target):\n",
    "    # Group data by target categories\n",
    "    groups = [column[target == category] for category in target.unique()]\n",
    "    \n",
    "    # Perform ANOVA (F-test)\n",
    "    stat, p = f_oneway(*groups)\n",
    "    \n",
    "    # Return the p-value\n",
    "    return p\n",
    "\n",
    "# Function to perform Kruskal-Wallis for non-normally distributed continuous variables\n",
    "def kruskal_test(column, target):\n",
    "    # Group data by target categories\n",
    "    groups = [column[target == category] for category in target.unique()]\n",
    "    \n",
    "    # Perform Kruskal-Wallis test\n",
    "    stat, p = kruskal(*groups)\n",
    "    \n",
    "    # Return the p-value\n",
    "    return p\n",
    "\n",
    "# Iterate over each column to check for continuous variables\n",
    "for column in df.select_dtypes(include=['float64', 'int64']).columns:  # Select only continuous columns\n",
    "    if column != 'Hypertension':  # Skip the target variable\n",
    "        # Perform ANOVA or Kruskal-Wallis test based on the distribution\n",
    "        p_value = None\n",
    "        skew_value = df[column].skew()  # Get the skewness of the column\n",
    "        \n",
    "        # Ensure skew_value is scalar (only one value for each column)\n",
    "        if isinstance(skew_value, float):  # Check that skew_value is a scalar\n",
    "            # Check if skewness is less than 1, indicating normal distribution\n",
    "            if abs(skew_value) < 1:  # If skew is not high, assume normal distribution for ANOVA\n",
    "                p_value = anova_test(df[column], df['Hypertension'])\n",
    "                test_type = \"ANOVA\"\n",
    "            else:\n",
    "                p_value = kruskal_test(df[column], df['Hypertension'])\n",
    "                test_type = \"Kruskal-Wallis\"\n",
    "        \n",
    "            # Check p-value to determine if the feature is statistically significant\n",
    "            if p_value < 0.05:\n",
    "                print(f\"The feature '{column}' is important using {test_type} (p < 0.05).\")\n",
    "            else:\n",
    "                print(f\"The feature '{column}' is not significant using {test_type} (p >= 0.05).\")\n",
    "        else:\n",
    "            print(f\"Skewness value for '{column}' is not a scalar. Skipping the test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Simulation for Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo Simulation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Number of simulations\n",
    "n_iterations = 100\n",
    "accuracy_scores = []\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Assuming 'X' is your feature set and 'y' is your target variable\n",
    "for _ in range(n_iterations):\n",
    "    # Randomly split the data\n",
    "    X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(X, y, test_size=0.2, random_state=np.random.randint(100))\n",
    "\n",
    "    # Identify numerical columns\n",
    "    numerical_columns = X_train_mc.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    # Apply scaling to numerical columns and one-hot encoding to categorical columns\n",
    "    column_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_columns),\n",
    "            ('cat', OneHotEncoder(), X_train_mc.select_dtypes(include=['object']).columns)\n",
    "        ])\n",
    "\n",
    "    # Fit and transform the training data and transform the test data\n",
    "    X_train_transformed = column_transformer.fit_transform(X_train_mc)\n",
    "    X_test_transformed = column_transformer.transform(X_test_mc)\n",
    "    \n",
    "    # Train the logistic regression model\n",
    "    model.fit(X_train_transformed, y_train_mc)\n",
    "    \n",
    "    # Make predictions and evaluate the accuracy\n",
    "    y_pred_mc = model.predict(X_test_transformed)\n",
    "    accuracy_scores.append(accuracy_score(y_test_mc, y_pred_mc))\n",
    "\n",
    "# Analyze the results\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "std_accuracy = np.std(accuracy_scores)\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_accuracy:.4f}\")\n",
    "print(f\"Standard Deviation of Accuracy: {std_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
